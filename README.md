Leveraging TensorFlow's tf.data API, I created efficient data pipelines for both training and validation datasets, including image loading, resizing, shuffling, batching, and augmentation for the training set. For the model architecture, I chose EfficientNetB2 as the backbone, with global average pooling and dense layers for classification. Compiling the model with the Adam optimizer and categorical cross-entropy loss, I trained it for 12 epochs, utilizing early stopping and model checkpointing for regularization
